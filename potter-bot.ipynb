{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":935855,"sourceType":"datasetVersion","datasetId":506221},{"sourceId":2775920,"sourceType":"datasetVersion","datasetId":1694347}],"dockerImageVersionId":30121,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Making a Harry Potter chat bot \n\n\n**To run this notebook, please use the [Google Colab version](https://colab.research.google.com/drive/1o5LxBspm-C28HQvXN-PRQavapDbm5WjG?usp=sharing) I made.** For some reason, Kaggle hides the input fields required to login to huggingface, making it impossible to complete the tutorial on Kaggle.\n\nAlmost all of the code for training this bot was made by [Mohamed Hassan](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb#scrollTo=THybpQmXhoxK). Their code has been adapted to suit the tutorial better.\n\nThe training data has been fetched from this [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https://www.kaggle.com/andradaolteanu) on [Kaggle](https://www.kaggle.com/)\n\nLet's get started!\n","metadata":{"id":"MMFOoHOaq96x"}},{"cell_type":"markdown","source":"### Install the Huggingface transformers module","metadata":{"id":"8EN-zZi6pHIB"}},{"cell_type":"code","source":"! pip -q install transformers","metadata":{"id":"WD6iOcTmoaxE","execution":{"iopub.status.busy":"2024-03-27T18:43:55.422595Z","iopub.execute_input":"2024-03-27T18:43:55.422971Z","iopub.status.idle":"2024-03-27T18:44:03.862555Z","shell.execute_reply.started":"2024-03-27T18:43:55.422892Z","shell.execute_reply":"2024-03-27T18:44:03.861530Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Import DialoGPT\nDialoGPT is a chatbot model made by microsoft. This will be the base for our RickBot.","metadata":{"id":"IXuRTjrJo5vk"}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nmodel_size = \"small\" \ntokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\nmodel = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")","metadata":{"id":"FSvzC1j7_Tr8","execution":{"iopub.status.busy":"2024-03-27T18:45:35.179143Z","iopub.execute_input":"2024-03-27T18:45:35.179522Z","iopub.status.idle":"2024-03-27T18:45:38.660637Z","shell.execute_reply.started":"2024-03-27T18:45:35.179487Z","shell.execute_reply":"2024-03-27T18:45:38.659925Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Chat with the untrained model","metadata":{"id":"45l8_zjlpD5B"}},{"cell_type":"code","source":"def chat(model, tokenizer, trained=False):\n    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n\n    for step in range(5):\n        message = input(\"MESSAGE: \")\n\n        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n            break\n\n        # encode the new user input, add the eos_token and return a tensor in Pytorch\n        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt')\n\n        # append the new user input tokens to the chat history\n        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n        # generated a response while limiting the total chat history to 1000 tokens, \n        if (trained):\n            chat_history_ids = model.generate(\n                bot_input_ids, \n                max_length=1000,\n                pad_token_id=tokenizer.eos_token_id,  \n                no_repeat_ngram_size=3,       \n                do_sample=True, \n                top_k=100, \n                top_p=0.7,\n                temperature = 0.8, \n            )\n        else:\n            chat_history_ids = model.generate(\n                bot_input_ids, \n                max_length=1000, \n                pad_token_id=tokenizer.eos_token_id,\n                no_repeat_ngram_size=3\n            )\n\n        # pretty print last ouput tokens from bot\n        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n\nchat(model, tokenizer)","metadata":{"id":"7NaCfs94pLw4","execution":{"iopub.status.busy":"2024-03-27T18:34:25.099511Z","iopub.execute_input":"2024-03-27T18:34:25.099918Z","iopub.status.idle":"2024-03-27T18:34:50.005702Z","shell.execute_reply.started":"2024-03-27T18:34:25.099879Z","shell.execute_reply":"2024-03-27T18:34:50.004771Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"type \"q\" to quit. Automatically quits after 5 messages\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  maaz\n"},{"name":"stdout","text":"DialoGPT: \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  hahha\n"},{"name":"stdout","text":"DialoGPT: \n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  where are u from?\n"},{"name":"stdout","text":"DialoGPT: I'm in the US and I've never heard of this.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  who are u\n"},{"name":"stdout","text":"DialoGPT: I've never seen this before.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  do u like me\n"},{"name":"stdout","text":"DialoGPT: I like you\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It's capable of holding a conversation, but doesn't resemble Rick Sanchez at all yet","metadata":{"id":"MIF90ucrhgFo"}},{"cell_type":"markdown","source":"## Configuring the model","metadata":{"id":"2Kj2BIaUiS71"}},{"cell_type":"code","source":"import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\nfrom typing import Dict, List, Tuple\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom tqdm.notebook import tqdm, trange\nfrom pathlib import Path\nfrom transformers import (\n    AdamW,\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\n    \ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\nlogger = logging.getLogger(__name__)\n\n# Args to allow for easy convertion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = f'output-{model_size}'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = f'microsoft/DialoGPT-{model_size}'\n        self.config_name = f'microsoft/DialoGPT-{model_size}'\n        self.tokenizer_name = f'microsoft/DialoGPT-{model_size}'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.per_gpu_train_batch_size = 4\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 10  # 3\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_total_limit = None\n        self.seed = 42\n        self.local_rank = -1\n\nargs = Args()","metadata":{"id":"jv9TXRvV1HIk","execution":{"iopub.status.busy":"2024-03-27T18:45:56.106297Z","iopub.execute_input":"2024-03-27T18:45:56.106662Z","iopub.status.idle":"2024-03-27T18:45:56.150184Z","shell.execute_reply.started":"2024-03-27T18:45:56.106632Z","shell.execute_reply":"2024-03-27T18:45:56.149419Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Gather the training data\n\nWe're using some rick and morty scripts from this [article](https://www.kaggle.com/andradaolteanu/sentiment-analysis-rick-and-morty-scripts/#1.-Data-%F0%9F%93%81) by [Andrada Olteanu](https://www.kaggle.com/andradaolteanu)  \\(the data can be found [here](https://www.kaggle.com/andradaolteanu/rickmorty-scripts)\\)","metadata":{"id":"aAES6tb6jh1O"}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/harry-potter-final-data/final_data.csv\")\n\n'''contexted = []\nn = 7\n\nfor i in range(n, len(data['line'])):\n  row = []\n  prev = i - 1 - n\n  for j in range(i, prev, -1):\n    row.append(data['line'][j])\n  contexted.append(row) \n\ncolumns = ['response'] + ['context '+str(i+1) for i in range(n)]\ndf = pd.DataFrame.from_records(contexted, columns=columns)'''\ndf.head(5)","metadata":{"id":"3dS1radujj2J","execution":{"iopub.status.busy":"2024-03-27T18:45:59.077161Z","iopub.execute_input":"2024-03-27T18:45:59.077533Z","iopub.status.idle":"2024-03-27T18:45:59.171264Z","shell.execute_reply.started":"2024-03-27T18:45:59.077498Z","shell.execute_reply":"2024-03-27T18:45:59.170334Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                            response  \\\n0  Do you think it wise to trust Hagrid with some...   \n1  Ah, Professor, I would trust Hagrid with my life.   \n2                         Professor Dumbledore, sir.   \n3                              Professor McGonagall.   \n4                      No problems, I trust, Hagrid?   \n\n                                            context1  \\\n0                            Hagrid is bringing him.   \n1  Do you think it wise to trust Hagrid with some...   \n2  Ah, Professor, I would trust Hagrid with my life.   \n3                         Professor Dumbledore, sir.   \n4                              Professor McGonagall.   \n\n                                            context2  \\\n0                                       And the boy?   \n1                            Hagrid is bringing him.   \n2  Do you think it wise to trust Hagrid with some...   \n3  Ah, Professor, I would trust Hagrid with my life.   \n4                         Professor Dumbledore, sir.   \n\n                                            context3  \\\n0                              The good and the bad.   \n1                                       And the boy?   \n2                            Hagrid is bringing him.   \n3  Do you think it wise to trust Hagrid with some...   \n4  Ah, Professor, I would trust Hagrid with my life.   \n\n                                            context4  \\\n0                          I'm afraid so, professor.   \n1                              The good and the bad.   \n2                                       And the boy?   \n3                            Hagrid is bringing him.   \n4  Do you think it wise to trust Hagrid with some...   \n\n                      context5                             context6  \\\n0  Are the rumors true, Albus?  Good evening, Professor Dumbledore.   \n1    I'm afraid so, professor.          Are the rumors true, Albus?   \n2        The good and the bad.            I'm afraid so, professor.   \n3                 And the boy?                The good and the bad.   \n4      Hagrid is bringing him.                         And the boy?   \n\n                                            context7  \n0  I should've known that you would be here, Prof...  \n1                Good evening, Professor Dumbledore.  \n2                        Are the rumors true, Albus?  \n3                          I'm afraid so, professor.  \n4                              The good and the bad.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>context1</th>\n      <th>context2</th>\n      <th>context3</th>\n      <th>context4</th>\n      <th>context5</th>\n      <th>context6</th>\n      <th>context7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Do you think it wise to trust Hagrid with some...</td>\n      <td>Hagrid is bringing him.</td>\n      <td>And the boy?</td>\n      <td>The good and the bad.</td>\n      <td>I'm afraid so, professor.</td>\n      <td>Are the rumors true, Albus?</td>\n      <td>Good evening, Professor Dumbledore.</td>\n      <td>I should've known that you would be here, Prof...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ah, Professor, I would trust Hagrid with my life.</td>\n      <td>Do you think it wise to trust Hagrid with some...</td>\n      <td>Hagrid is bringing him.</td>\n      <td>And the boy?</td>\n      <td>The good and the bad.</td>\n      <td>I'm afraid so, professor.</td>\n      <td>Are the rumors true, Albus?</td>\n      <td>Good evening, Professor Dumbledore.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Professor Dumbledore, sir.</td>\n      <td>Ah, Professor, I would trust Hagrid with my life.</td>\n      <td>Do you think it wise to trust Hagrid with some...</td>\n      <td>Hagrid is bringing him.</td>\n      <td>And the boy?</td>\n      <td>The good and the bad.</td>\n      <td>I'm afraid so, professor.</td>\n      <td>Are the rumors true, Albus?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Professor McGonagall.</td>\n      <td>Professor Dumbledore, sir.</td>\n      <td>Ah, Professor, I would trust Hagrid with my life.</td>\n      <td>Do you think it wise to trust Hagrid with some...</td>\n      <td>Hagrid is bringing him.</td>\n      <td>And the boy?</td>\n      <td>The good and the bad.</td>\n      <td>I'm afraid so, professor.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>No problems, I trust, Hagrid?</td>\n      <td>Professor McGonagall.</td>\n      <td>Professor Dumbledore, sir.</td>\n      <td>Ah, Professor, I would trust Hagrid with my life.</td>\n      <td>Do you think it wise to trust Hagrid with some...</td>\n      <td>Hagrid is bringing him.</td>\n      <td>And the boy?</td>\n      <td>The good and the bad.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T18:46:02.782162Z","iopub.execute_input":"2024-03-27T18:46:02.782541Z","iopub.status.idle":"2024-03-27T18:46:02.788074Z","shell.execute_reply.started":"2024-03-27T18:46:02.782507Z","shell.execute_reply":"2024-03-27T18:46:02.787114Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"4918"},"metadata":{}}]},{"cell_type":"markdown","source":"\nWe want the model to be aware of previous messages from the dialogue to help it decide what to say next. Here we modify the dataset to include context from 7 previous messages.","metadata":{"id":"de1R3f3-kGwY"}},{"cell_type":"markdown","source":"### Formatting the data and defining some helper functions\nWe need to construct the data in the right format so the bot can interpret it properly. To do this we're adding special characters like the 'end of string' charater\n","metadata":{"id":"5gRD9mhKkcT9"}},{"cell_type":"code","source":"def construct_conv(row, tokenizer, eos = True):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n    conv = flatten(conv)\n    return conv\n\ndef load_and_cache_examples(args, tokenizer, df_trn):\n    return ConversationDataset(tokenizer, args, df_trn)\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n        directory = args.cache_dir\n        cached_features_file = os.path.join(directory, args.model_type + \"_cached_lm_\" + str(block_size))\n\n        logger.info(\"Creating features from dataset file at %s\", directory)\n        self.examples = []\n        for _, row in df.iterrows():\n            conv = construct_conv(row, tokenizer)\n            self.examples.append(conv)\n\n        logger.info(\"Saving features into cached file %s\", cached_features_file)\n        with open(cached_features_file, \"wb\") as handle:\n            pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)","metadata":{"id":"mdjT5EqKkwZb","execution":{"iopub.status.busy":"2024-03-27T18:46:12.942539Z","iopub.execute_input":"2024-03-27T18:46:12.943010Z","iopub.status.idle":"2024-03-27T18:46:12.954818Z","shell.execute_reply.started":"2024-03-27T18:46:12.942968Z","shell.execute_reply":"2024-03-27T18:46:12.953791Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\nNow, this is quite a hefty chunk of code but don't worry you don't need to understant it yet, we can cover this in later tutorials","metadata":{"id":"83oT-xHu4msu"}},{"cell_type":"code","source":"def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    logger.info(\"*** Running trainng, Num examples = %d, Num Epochs = %d ***\", len(train_dataset), args.num_train_epochs)\n\n    global_step, epochs_trained = 0, 0\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n            \n            inputs, labels = (batch, batch)\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n    tb_writer.close()\n\n    return global_step, tr_loss / global_step","metadata":{"id":"6W9ZUG-14pI_","execution":{"iopub.status.busy":"2024-03-27T18:46:28.025121Z","iopub.execute_input":"2024-03-27T18:46:28.025495Z","iopub.status.idle":"2024-03-27T18:46:28.045158Z","shell.execute_reply.started":"2024-03-27T18:46:28.025460Z","shell.execute_reply":"2024-03-27T18:46:28.044245Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Main Runner\n\nHere we're simply setting up the logger and starting the training!","metadata":{"id":"vWjTu6fI4yP8"}},{"cell_type":"code","source":"def main(df_trn):\n    args = Args()\n    \n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.n_gpu = torch.cuda.device_count()\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s\", args.local_rank, device, args.n_gpu)\n\n    set_seed(args) # Set seed\n\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path, from_tf=False, config=config, cache_dir=args.cache_dir)\n    model.to(args.device)\n    \n    # Training\n    train_dataset = load_and_cache_examples(args, tokenizer, df_trn)\n    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n    model_to_save.save_pretrained(args.output_dir)\n    tokenizer.save_pretrained(args.output_dir)\n\n    # Good practice: save your training arguments together with the trained model\n    torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n    # Load a trained model and vocabulary that you have fine-tuned\n    model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n    model.to(args.device)","metadata":{"id":"Jludg4aN4zdc","execution":{"iopub.status.busy":"2024-03-27T18:46:38.789106Z","iopub.execute_input":"2024-03-27T18:46:38.789475Z","iopub.status.idle":"2024-03-27T18:46:38.800206Z","shell.execute_reply.started":"2024-03-27T18:46:38.789441Z","shell.execute_reply":"2024-03-27T18:46:38.799329Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Lets Run it!\nThis should take around 5 minutes so you might as well go grab a cup of coffee ☕️","metadata":{"id":"ApbF-p305CYv"}},{"cell_type":"code","source":"main(df)","metadata":{"id":"sfTdpQy-5D1n","execution":{"iopub.status.busy":"2024-03-27T18:46:45.457862Z","iopub.execute_input":"2024-03-27T18:46:45.458194Z","iopub.status.idle":"2024-03-27T19:08:02.004719Z","shell.execute_reply.started":"2024-03-27T18:46:45.458165Z","shell.execute_reply":"2024-03-27T19:08:02.003781Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc23cc9eed3146dfbedd9cc7a5974fef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6fd012cf10f4991ab683a3e71bd4d51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b9bb7974914a54aeb6b43ea05f7fc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"691840bb5ab240df93e3b79e82eceeb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/351M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6e5f76cfad4e418469d28f824a0aa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea88f397fd504cb48e48372eac0e9cc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"460df83fe605411098c1d05de343a708"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"212b6d80f5da4f95a8a38ceda698ea9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"514e902809984869baf55e17b532cf8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f601d18aec144084bb5d94047aecf6f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd7c30dd4c54d8ca8f6ab57914cdd29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"750985a108b34832a5398fd0ed4eb229"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ad7556014a4875adb63fdce7ac8645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7af7e97abf448b99117d79a175bf91a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01dd5b5873b464f91d6931da6182cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1229 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06c2e8ab6a0d4812b65c68d73bf0ef23"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Chatting with the trained bot","metadata":{"id":"4xYlHoEB5Jic"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(f'microsoft/DialoGPT-{model_size}')\nmodel = AutoModelForCausalLM.from_pretrained(f'output-{model_size}')\nchat(model, tokenizer, trained=True)","metadata":{"id":"NkZ0yjsc5LX-","execution":{"iopub.status.busy":"2024-03-27T02:05:17.877219Z","iopub.execute_input":"2024-03-27T02:05:17.877624Z","iopub.status.idle":"2024-03-27T02:06:09.461062Z","shell.execute_reply.started":"2024-03-27T02:05:17.877591Z","shell.execute_reply":"2024-03-27T02:06:09.459352Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"type \"q\" to quit. Automatically quits after 5 messages\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  How are u Harry?\n"},{"name":"stdout","text":"DialoGPT: I'm fine. Go.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  Where do you live?\n"},{"name":"stdout","text":"DialoGPT: This is Scabbers, by the way.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  Who are u?\n"},{"name":"stdout","text":"DialoGPT: Lobby sir, Lobby the house elf.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"MESSAGE:  Whats your name?\n"},{"name":"stdout","text":"DialoGPT: Oh, sorry sir. I’m Harry, sir, Harry Potter.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-c40408955cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'microsoft/DialoGPT-{model_size}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'output-{model_size}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-b15c3b612564>\u001b[0m in \u001b[0;36mchat\u001b[0;34m(model, tokenizer, trained)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MESSAGE: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"q\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if the user doesn't wanna talk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         )\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}]},{"cell_type":"markdown","source":"That's more like it!","metadata":{"id":"_vFiJlFZwuPF"}},{"cell_type":"markdown","source":"# Uploading to HuggingFace 🤗\n\nHuggingFace is a platform for hosting machine learning models. Think Github for ML. \n\nWe're going to use it to host our new bot so that it can be accessed from anywhere, even after this google colab session ends.","metadata":{"id":"hZVKRKULw1OM"}},{"cell_type":"code","source":"!apt-get install git-lfs","metadata":{"id":"fVnt8HGS3XwY","execution":{"iopub.status.busy":"2024-03-27T15:09:34.024300Z","iopub.execute_input":"2024-03-27T15:09:34.024599Z","iopub.status.idle":"2024-03-27T15:09:38.016994Z","shell.execute_reply.started":"2024-03-27T15:09:34.024527Z","shell.execute_reply":"2024-03-27T15:09:38.015947Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  git-lfs\n0 upgraded, 1 newly installed, 0 to remove and 12 not upgraded.\nNeed to get 2129 kB of archives.\nAfter this operation, 7662 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2129 kB]\nFetched 2129 kB in 0s (18.9 MB/s)\ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package git-lfs.\n(Reading database ... 102229 files and directories currently installed.)\nPreparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\nUnpacking git-lfs (2.3.4-1) ...\nSetting up git-lfs (2.3.4-1) ...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Git needs an email address:","metadata":{"id":"9ifVP6WroFib"}},{"cell_type":"code","source":"userEmail = input(\"Enter git email: \")\n!git config --global user.email \"$userEmail\" ","metadata":{"id":"Hl21ldCcnDIy","execution":{"iopub.status.busy":"2024-03-27T02:06:55.492547Z","iopub.execute_input":"2024-03-27T02:06:55.493026Z","iopub.status.idle":"2024-03-27T02:06:59.427671Z","shell.execute_reply.started":"2024-03-27T02:06:55.492983Z","shell.execute_reply":"2024-03-27T02:06:59.426360Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter git email:  maazullahminhas96@gmail.com\n"}]},{"cell_type":"markdown","source":"Login with your huggingface account, if you don't have one you can sign up [here](https://huggingface.co/)","metadata":{"id":"y50Xqym82bx8"}},{"cell_type":"raw","source":"! huggingface-cli login  ","metadata":{"execution":{"iopub.status.busy":"2021-08-10T14:15:37.624415Z","iopub.execute_input":"2021-08-10T14:15:37.624878Z"}}},{"cell_type":"code","source":"model_name = \"RickBotExample\" # you can change this, make sure it doesn't contain any spaces though\n\nconversational_tag = \"\"\"---\ntags:\n- conversational\n---\n# RickBot built for [Chai](https://chai.ml/)\nMake your own [here](https://colab.research.google.com/drive/1LtVm-VHvDnfNy7SsbZAqhh49ikBwh1un?usp=sharing)\"\"\"\n\nmodel.push_to_hub(model_name)\n! echo \"$conversational_tag\" > \"$model_name/README.md\"\ntokenizer.push_to_hub(model_name)\n\n! rm -r \"$model_name/\"   # clean up local directory","metadata":{"id":"JT9O8EkD2svv","execution":{"iopub.status.busy":"2021-08-10T13:17:12.682661Z","iopub.execute_input":"2021-08-10T13:17:12.683052Z","iopub.status.idle":"2021-08-10T13:17:13.571259Z","shell.execute_reply.started":"2021-08-10T13:17:12.683017Z","shell.execute_reply":"2021-08-10T13:17:13.568638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can copy and paste the url above into your browser to see your new bot's page on huggingface\n\nGreat! Now our bot is being hosted on HuggingFace we can deploy to Chai","metadata":{"id":"YfBpCJdWgoT4"}},{"cell_type":"markdown","source":"# Deploying to Chai\n\n<img src=\"https://i.imgur.com/IjZ12pt.png\" width=\"500\">\n\nChai is a platform for creating, sharing and interacting with conversational AI's. It allows us to chat with our new bot through a mobile app. This means you can show it off really easily, no need to whip out your laptop and fire up a colab instance, simply open the app and get chatting!\n\nThere is also a bot leaderboard to climb. We can see how our new bot compares to others on the platform:\n\n<img src=\"https://i.imgur.com/ctPYQVZ.png\" width=\"850\">\n\nLets deploy RickBot to Chai!\n","metadata":{"id":"kx6TdIB1w3E6"}},{"cell_type":"markdown","source":"Install the chaipi package:","metadata":{"id":"n5_ve-Mrca7f"}},{"cell_type":"code","source":"!pip install --upgrade chaipy","metadata":{"id":"TiQIVJJwcHfi","execution":{"iopub.status.busy":"2021-08-10T13:17:13.572374Z","iopub.status.idle":"2021-08-10T13:17:13.572858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setup the notebook:","metadata":{"id":"zLBVX5HQchDd"}},{"cell_type":"code","source":"import chai_py\nchai_py.setup_notebook()","metadata":{"id":"4IYwxLmfcaMO","execution":{"iopub.status.busy":"2021-08-10T13:17:13.574221Z","iopub.status.idle":"2021-08-10T13:17:13.574973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Head over to the [Chai Dev Platform](https://chai.ml/dev/?utm_source=kaggle&utm_medium=social&utm_campaign=kaggle) to set up your developer account. This allows us to deploy the bot under our own account\n\nYour developer ID and keys can be found on the [dev page](https://chai.ml/dev/?utm_source=kaggle&utm_medium=social&utm_campaign=kaggle)","metadata":{"id":"OlaFXRsjckgg"}},{"cell_type":"code","source":"from chai_py.auth import set_auth\n\nDEV_UID = input(\"Enter dev UID: \")\nDEV_KEY = input(\"Enter dev key: \")\nset_auth(DEV_UID, DEV_KEY)","metadata":{"id":"i7Lzej2nc1_1","execution":{"iopub.status.busy":"2021-08-10T13:17:13.576734Z","iopub.status.idle":"2021-08-10T13:17:13.577614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_name = !huggingface-cli whoami\nuserPlusModel = f\"{user_name[0]}/{model_name}\"\n%store userPlusModel > bot/myArguments.txt","metadata":{"id":"Xxs9aQWiQUE5","execution":{"iopub.status.busy":"2021-08-10T13:17:13.579334Z","iopub.status.idle":"2021-08-10T13:17:13.580251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chai bot code\n\nYou can add to the `past_user_inputs` to change the bot's starting context. This changes how the bot will act when the conversation starts.","metadata":{"id":"jzKdzMVSe0PH"}},{"cell_type":"code","source":"%%write_and_run bot bot.py Bot\nimport json\nimport requests\nimport time\nfrom chai_py import ChaiBot, Update\n\nf = open(\"myArguments.txt\", \"r\")\nuserPlusModel = f.read()\n\nclass Bot(ChaiBot):\n    \n    def setup(self):\n        self.ENDPOINT = f\"https://api-inference.huggingface.co/models/{userPlusModel}\"\n        self.headers = { \"Authorization\": \"Bearer api_oieZbocfGuGxzuQozzaqpFYnBrpBsSLwzP\" }\n        self.first_response = \"Hey, I'm Rick\" # you can change this\n\n    async def on_message(self, update: Update) -> str:\n        if update.latest_message.text == self.FIRST_MESSAGE_STRING:\n            return self.first_response\n        payload = await self.get_payload(update)\n        return self.query(payload)\n\n    def query(self, payload):\n        data = json.dumps(payload)\n        response = requests.post(self.ENDPOINT, headers=self.headers, data=data)\n\n        if (response.status_code == 503):  # This means we need to wait for the model to load 😴.\n            estimated_time = response.json()[\"estimated_time\"]\n            time.sleep(estimated_time)\n            data = json.loads(data)\n            data[\"options\"] = {\"use_cache\": False, \"wait_for_model\": True}\n            data = json.dumps(data)\n            response = requests.post(self.ENDPOINT, headers=self.headers, data=data)\n\n        return json.loads(response.content.decode(\"utf-8\"))[\"generated_text\"]\n\n    async def get_payload(self, update):\n        past_user_inputs = [\"Hey\"]  # You can add to this!\n        generated_responses = [self.first_response]  # and this!\n        return {\n            \"inputs\": {\n                \"past_user_inputs\": past_user_inputs,\n                \"generated_responses\": generated_responses,\n                \"text\": update.latest_message.text,\n            },\n        }","metadata":{"id":"F3ktc9LUe-F-","execution":{"iopub.status.busy":"2021-08-10T13:17:13.582036Z","iopub.status.idle":"2021-08-10T13:17:13.582819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deploy to Chai\nTime to deploy the bot!\n\nYou can change the `image_url` and `description` to personalise how the bot will appear on the platform","metadata":{"id":"i1ADjwIDlEXT"}},{"cell_type":"code","source":"from chai_py import package, Metadata, upload_and_deploy, wait_for_deployment, share_bot\n\npackage(\n    Metadata(\n        name=model_name,\n        image_url=\"https://live.staticflickr.com/65535/48185490292_1896035611_b.jpg\",\n        color=\"0000ff\",\n        description=\"Pickle Rick!\",\n        input_class=Bot,\n        developer_uid=DEV_UID,\n        memory=3000,\n    )\n)\n\nprint()\nbot_uid = upload_and_deploy(\"bot/_package.zip\")\nwait_for_deployment(bot_uid)\nshare_bot(bot_uid)","metadata":{"id":"u1qprkX3lJFs","execution":{"iopub.status.busy":"2021-08-10T13:17:13.584516Z","iopub.status.idle":"2021-08-10T13:17:13.58549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Success 🎉\nScan the QR code above with your phone and you will be taken to a chat screen with your brand new bot, how cool is that?!\n\n(Make sure you have the Chai app installed on your phone, get it on the [App store](https://apps.apple.com/us/app/chai/id1544750895) or [Google Play](https://play.google.com/store/apps/details?id=com.Beauchamp.Messenger.external&pcampaignid=pcampaignidMKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1))","metadata":{"id":"952b_duKglFn"}},{"cell_type":"markdown","source":"# Conclusion\n\nAny dialogue can be used to trian this bot, it just needs to be in the right format. Try making a bot of your favourite film character, or maybe from a TV show.\n\nYou can also try changing the training config. For example, the context length (`n`), or any of the arguments in the `Args` class.","metadata":{"id":"8K4pSp4jg0us"}},{"cell_type":"markdown","source":"**More tutorials:**\n- **[Chai docs](https://chai.ml/docs/?utm_source=kaggle&utm_medium=social&utm_campaign=kaggle) creating your first bot**\n- **[Deploying a pretrained chatbot on Chai](https://www.kaggle.com/rsedlr/deploying-a-chatbot)**\n- **[Making a Rock Paper Scissors AI chat bot with Chai](https://www.kaggle.com/rsedlr/making-a-rock-paper-scissors-ai-chatbot-with-chai)**","metadata":{}}]}