{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HQO_5iCFjS4I"
      },
      "outputs": [],
      "source": [
        "! pip -q install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_size = \"small\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(f\"microsoft/DialoGPT-{model_size}\")"
      ],
      "metadata": {
        "id": "A_pE5bU6jUBt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(model, tokenizer, trained=False):\n",
        "    print(\"type \\\"q\\\" to quit. Automatically quits after 5 messages\")\n",
        "\n",
        "    for step in range(5):\n",
        "        message = input(\"MESSAGE: \")\n",
        "\n",
        "        if message in [\"\", \"q\"]:  # if the user doesn't wanna talk\n",
        "            break\n",
        "\n",
        "        # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "        new_user_input_ids = tokenizer.encode(message + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "        # append the new user input tokens to the chat history\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "        # generated a response while limiting the total chat history to 1000 tokens,\n",
        "        if (trained):\n",
        "            chat_history_ids = model.generate(\n",
        "                bot_input_ids,\n",
        "                max_length=1000,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3,\n",
        "                do_sample=True,\n",
        "                top_k=100,\n",
        "                top_p=0.7,\n",
        "                temperature = 0.8,\n",
        "            )\n",
        "        else:\n",
        "            chat_history_ids = model.generate(\n",
        "                bot_input_ids,\n",
        "                max_length=1000,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "        # pretty print last ouput tokens from bot\n",
        "        print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "\n",
        "# chat(model, tokenizer)"
      ],
      "metadata": {
        "id": "2g2B5yIkjVwe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(f'microsoft/DialoGPT-{model_size}',padding_side='left')"
      ],
      "metadata": {
        "id": "QemZXaqAjXWR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(f'/content/drive/MyDrive/DialogPT')"
      ],
      "metadata": {
        "id": "P3w8cSY-jZPK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat(model, tokenizer, trained=True)"
      ],
      "metadata": {
        "id": "dD-DNeRMkEQ1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import torch\n",
        "\n",
        "def chat_with_bleu_scores(model, tokenizer, questions, expected_responses, trained=False):\n",
        "    bleu_scores = []\n",
        "\n",
        "    for question, expected_response in zip(questions, expected_responses):\n",
        "        print(f\"Question: {question}\")\n",
        "\n",
        "        new_user_input_ids = tokenizer.encode(question + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "        bot_input_ids = new_user_input_ids\n",
        "\n",
        "        if trained:\n",
        "            chat_history_ids = model.generate(\n",
        "                bot_input_ids,\n",
        "                max_length=1000,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3,\n",
        "                do_sample=True,\n",
        "                top_k=100,\n",
        "                top_p=0.7,\n",
        "                temperature = 0.8,\n",
        "            )\n",
        "        else:\n",
        "            chat_history_ids = model.generate(\n",
        "                bot_input_ids,\n",
        "                max_length=1000,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3\n",
        "            )\n",
        "\n",
        "        generated_response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "        print(f\"DialoGPT: {generated_response}\")\n",
        "\n",
        "        reference = [expected_response.split()]\n",
        "        candidate = generated_response.split()\n",
        "        bleu_score = sentence_bleu(reference, candidate)\n",
        "        bleu_scores.append(bleu_score)\n",
        "        # print(f\"BLEU score: {bleu_score}\\n\")\n",
        "    return bleu_scores\n",
        "\n",
        "# Example usage:\n"
      ],
      "metadata": {
        "id": "90Wr1KdTkHu_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"We're going to the park\",\n",
        "    \"What are goblins\",\n",
        "    \"This is what we're going to do...\",\n",
        "    \"Can you hear me?\",\n",
        "    # \"Do you miss your family?\",\n",
        "    # \"Excuse me Sir\"\n",
        "    # \"You’ll stay with me? \",\n",
        "    # \"where have you come from?\" ,\n",
        "    # \"What is it with her?\"\n",
        "    ]\n",
        "\n",
        "expected_responses = [\n",
        "    \"Here he comes, the birthday boy.\",\n",
        "    \"goblins but not the most friendly of beasts , have a look at this \",\n",
        "    \"is that when we go out, we're going to buy you two new presents. Come on, why potter “I’m coming with you,”\",\n",
        "    \"It's just, I've never talked to a snake before. Oh yeah  ,No i could not  \",\n",
        "    # \"Yes I see. That's me as well. potter yes,i miss\",\n",
        "    # \"Rubeus Hagrid, Keeper of Keys and Grounds at Hogwarts., Er — okay, ,,potter, yes right dear\" ,\n",
        "    # \"We'll stay with you too. Maybe I’d better stay here,“Let me go!” have they got away space\",\n",
        "    # \"a really good question and Me? of course i think this time this drink professor \",\n",
        "    # \"Nothing , she is freak It’s my  friend and she used my wand before and always contains lipstick \"\n",
        "    ]\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "EQlMLDCWkL9W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_scores =chat_with_bleu_scores(model, tokenizer, questions, expected_responses, trained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gT5aApUkNz1",
        "outputId": "55614767-333c-412c-8cab-46b35b1f4e3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: We're going to the park\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DialoGPT: Here he comes, the birthday boy.\n",
            "Question: What are goblins\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DialoGPT: Clever as they come goblins but not the most friendly of beasts.\n",
            "Question: This is what we're going to do...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DialoGPT: is that when we go out, we're gonna buy you two new presents.\n",
            "Question: Can you hear me?\n",
            "DialoGPT: It's just, I've never talked to a snake before.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_bleu_score = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "print(f\"Average BLEU score across the dataset: {average_bleu_score}\")"
      ],
      "metadata": {
        "id": "5YrNd5KskRDQ",
        "outputId": "d4aa7bf0-baf4-4386-95b6-0ff56c7b4116",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score across the dataset: 0.5827040227176832\n"
          ]
        }
      ]
    }
  ]
}